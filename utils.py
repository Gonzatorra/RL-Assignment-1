import os
import re
import time
import json
import optuna
import numpy as np
import matplotlib.pyplot as plt
from D_A_N_I_env import DaniEnv #import for analyze_reward_sensitivity
import pandas as pd
import seaborn as sns


#-----------------------------------------------#
#----------CREATE THE MAIN FOLDER---------------#
#-----------------------------------------------#
save_dir = "plots"
os.makedirs(save_dir, exist_ok=True)





#-----------------------------------------------#
#----------COMPUTE AND LOAD STATISTICS----------#
#-----------------------------------------------#


#----------SAVE RESULTS IN TXT FILE----------#
def save_results_txt(algorithm_name, grid_idx, variant, mean_last100, std_last100, mean_len, elapsed, mean_rewards, std_rewards):
    """
    Saves statistics for a given algorithm and grid in a TXT file.

    Args:
        algorithm_name (str): Algorithm name
        grid_idx (int): Index of the grid
        variant (str): 'no_decay' or 'with_decay'
        mean_last100 (float): Average of rewards for the last 100 episodes
        std_last100 (float): Standard deviation of rewards for the last 100 episodes
        mean_len (float): Steps mean for each episode
        elapsed (float): Needed time for compute it
        mean_rewards (np.array): Mean reward curve
        std_rewards (np.array): Std of reward curve

    Saves:
        A '.txt' file named 'results_summary_{algorithm_name}.txt' showing
        the general statistics for a given algoriths on a given grid.

    Example:
        save_results_txt("q_learning", 0, "with_decay", 107.7, 10.5, 40, 0.4s)
        File saved as "results_summary_q_learning"
    """

    #Create the folder for plots if it does not exist
    folder_plots  = f"plots/{algorithm_name}"
    os.makedirs(folder_plots , exist_ok=True)
    output_file = os.path.join(folder_plots , f"results_summary_{algorithm_name}.txt")

    #Create the folder for data if it does not exist
    folder_data = os.path.join(folder_plots, "data")
    os.makedirs(folder_data, exist_ok=True)

    #Write all the statistics in the file
    with open(output_file, "a") as f:
        f.write(
            f"Grid {grid_idx} - {variant}: "
            f"mean_last100={mean_last100:.2f}, std={std_last100:.2f}, "
            f"mean_length={mean_len:.1f}, time={elapsed:.1f}s\n\n"
        )

    #Write all the curves in .npz to plot them
    np.savez(
        os.path.join(folder_data, f"{algorithm_name}_grid{grid_idx}_{variant}.npz"),
        mean_rewards=mean_rewards,
        std_rewards=std_rewards
    )



#----------LOAD REWARDS FORM .NPZ----------#
def load_rewards(algorithm_name, grid_idx, variant):
    """
    Loads the mean and standard deviation reward curves for a given algorithm, grid, and variant from a previously saved .npz file.
    
    Args:
        algorithm_name (str): Name of the algorithm (e.g., "q_learning", "montecarlo", "SARSA").
        grid_idx (int): Index of the grid for which the data was saved.
        variant (str): Variant name, typically 'no_decay' or 'with_decay'.

    Returns:
        tuple[np.ndarray, np.ndarray]: 
            - mean_rewards: 1D array of mean rewards per episode.
            - std_rewards: 1D array of standard deviation of rewards per episode.
    """
    folder = f"plots/{algorithm_name}/data"
    data = np.load(os.path.join(folder, f"{algorithm_name}_grid{grid_idx}_{variant}.npz"))
    return data['mean_rewards'], data['std_rewards']




#----------READ RESULTS FROM TXT FILE----------#
def read_summary_file(algorithm_name):
    """
    Reads general results of a given txt generated by save_results_txt.
    
    Args:
        algorithm_name (str): Algorithm name

    Returns:
        list[dict]: Dictionary list with all the information for each grid and variant of a given algorithm
    """

    #Get the path for read the statistics
    path = f"plots/{algorithm_name}/results_summary_{algorithm_name}.txt"
    data = []

    #Read all the information of the txt file
    with open(path, "r") as f:
        for line in f:
            match = re.match(
                r"Grid (\d+) - (\w+): mean_last100=(-?\d+(?:\.\d+)?), std=(-?\d+(?:\.\d+)?), mean_length=(-?\d+(?:\.\d+)?), time=(-?\d+(?:\.\d+)?)s",
                line
            )
            if match:
                grid_idx, variant, mean, std, length, time_sec = match.groups()
                data.append({
                    "grid_idx": int(grid_idx),
                    "variant": variant,
                    "mean_last100": float(mean),
                    "std_last100": float(std),
                    "mean_length": float(length),
                    "time": float(time_sec)
                })
    return data



def summarize_sensitivity_results(results_df):
    """
    Generate a statistical summary of the sensitivity analysis results.
    
    For each algorithm, identifies the best configuration based on the highest 
    mean reward, displaying key performance metrics such as average reward, 
    standard deviation, success rate, and mean number of steps. Also determines 
    the overall best configuration across all algorithms and computes the 
    correlation between different reward components and the mean reward.

    Args:
        results_df (pd.DataFrame): DataFrame containing the sensitivity analysis 
            results. Must include at least the following columns:
            ['algorithm', 'config', 'mean_reward', 'std_reward', 
             'success_rate', 'mean_steps', 'meteorite_reward', 
             'palm_reward', 'goal_reward', 'step_reward'].

    Prints:
        - The best configuration per algorithm (with mean reward, standard deviation, 
          success rate, and mean steps).
        - The overall best configuration across all algorithms.
        - Correlation coefficients between each reward component and the mean reward,
          along with qualitative significance indicators (*, **, ***).
    """
    if results_df is None or results_df.empty:
        print("No hay resultados para resumir")
        return

    #Best configurations by algorithm
    print("\nBest configurations by algorithm:")
    print("-" * 50)
    
    for algo in results_df['algorithm'].unique():
        algo_data = results_df[results_df['algorithm'] == algo]
        best_config = algo_data.loc[algo_data['mean_reward'].idxmax()]
        
        print(f"\n{algo.upper():<12}")
        print(f"  Best configuration: {best_config['config']}")
        print(f"  Average reward: {best_config['mean_reward']:.2f} ± {best_config['std_reward']:.2f}")
        print(f"  Success rate: {best_config['success_rate']:.1%}")
        print(f"  Mean steps: {best_config['mean_steps']:.1f}")

    #Overall best configuration
    overall_best = results_df.loc[results_df['mean_reward'].idxmax()]
    print(f"\n{'OVERALL BEST CONFIGURATION':<40}")
    print(f"  Configuration: {overall_best['config']}")
    print(f"  Algorithm: {overall_best['algorithm']}")
    print(f"  Reward: {overall_best['mean_reward']:.2f}")

    #Correlation analysis
    print(f"\n{'CORRELATION REWARDS vs PERFORMANCE':<40}")
    print("-" * 50)
    
    for reward_type in ['meteorite_reward', 'palm_reward', 'goal_reward', 'step_reward']:
        correlation = results_df[reward_type].corr(results_df['mean_reward'])
        significance = "***" if abs(correlation) > 0.5 else "**" if abs(correlation) > 0.3 else "*" if abs(correlation) > 0.1 else ""
        
        reward_name = reward_type.replace('_reward', '').title()
        print(f"  {reward_name:<12}: r = {correlation:7.3f} {significance}")








#-----------------------------------------------#
#-----------------PAINTING PLOTS----------------#
#-----------------------------------------------#

#----------PLOT POLICY----------#
def plot_policy(grid, Q, plot_name, grid_idx):
    """
    Generates a visualization of the learned optimal policy.

    Args:
        grid (np.array): The environment (matrix)
        Q (np.array): Trained Q table
        plot_name (str): Name for saving the PNG file
        grid_idx (int): Grid index for a better understanding

    Saves:
        A '.png' file named 'algo_name_grid{grid_idx}_policy.png' showing
        the optimal policy for a given grid and algorithm.

    Example:
        plot_policy(gird_idx[0], Q_table, montecarlo_grid0.png, grid_idx=0)
        Plot saved as "montecarlo_grid0_policy.png"
    """

    #Name for the base folder
    base_folder = "plots/policy"
    #And the subfolders for each grid
    folder = os.path.join(base_folder, f"grid{grid_idx}")
    os.makedirs(folder, exist_ok=True)

    #All possible actions to take
    action_arrows = {
        0: '↑',
        1: '→',
        2: '↓',
        3: '←' 
    }
    
    #Get the grid size and create the plot
    grid_size = grid.shape[0]
    fig, ax = plt.subplots(figsize=(8,8))

    #Plot the grid
    for i in range(grid_size):
        for j in range(grid_size):
            cell = grid[i, j]
            rect = plt.Rectangle((j, i), 1, 1, facecolor='white', edgecolor='black')
            ax.add_patch(rect)

            if cell == 'N': #Nothing
                rect.set_facecolor("#72B842")
            elif cell == 'M': #Meteorite
                rect.set_facecolor("#5EAAF6")
            elif cell == 'P': #Palm
                rect.set_facecolor('#397716')
            else: #Start and goal
                rect.set_facecolor('#000000')

            #Get the optimal policy
            state = i * grid_size + j
            best_action = np.argmax(Q[state])
            arrow = action_arrows[best_action]

            #Paint the arrows to show the optimal policy
            ax.text(j + 0.5, i + 0.5, arrow, ha='center', va='center', fontsize=18, color='red')

    #Extra settings for the plot
    ax.set_xlim(0, grid_size)
    ax.set_ylim(0, grid_size)
    ax.set_aspect('equal')
    ax.invert_yaxis()
    plt.title("Optimal policy Grid " + str(grid_idx), fontsize=16)

    #Save each plot in the folder
    plot_path = os.path.join(folder, plot_name + ".png")
    plt.savefig(plot_path, bbox_inches='tight')
    plt.close(fig)
    print(f"Policy saved as {plot_path}")





#----------PLOT REWARDS/ALGORITHMS----------#
def smooth(x, window):
    """Returns a smoother curve of the rewards"""
    return [np.mean(x[max(0, i-window):i+1]) for i in range(len(x))]



def plot_rewards_comparison(algos_results, grid_idx, window):
    """Generates and saves a plot for the comparison of the different algorithms and their rewards.
    
    Args:
        algos_results (dict): Dictionary containing the results of each algoritms.
            - Each key is the algorithm name
            - Each value is another dictionary where:
                - 'Q': Trained Q table for the algorithm
                - 'rewards': Episode reward

        grid_idx (int): Grid index for a better understanding
        
        window (int): Size of the moving average window used to smooth the reard curves.

    Saves:
        A '.png' file named 'rewards_comparison_grid{grid_idx}.png' showing
        smoothed reward curves for all algorithms on the given grid.

    Example:
        plot_rewards_comparison(results_algos, grid_idx=0, window=500)
        Plot saved as "rewards_comparison_grid0.png"
    """
    

    plt.figure(figsize=(10, 6))

   #For each grid get each algorithm's data and plot it  
    for algo_name, data in algos_results.items():
        plt.plot(smooth(data['rewards'], window), label=algo_name)
    plt.xlabel("Episode")
    plt.ylabel(f"Total reward (last average {window})")
    plt.title(f"Grid {grid_idx} - Comparison between algorithms")
    plt.legend()
    plt.grid(True)

    #Base folder and subfolder for each grid
    base_folder = "plots/rewards"
    folder = os.path.join(base_folder, f"grid{grid_idx}")
    os.makedirs(folder, exist_ok=True)

    #Save the plot
    plot_path = os.path.join(folder, f"rewards_comparison_grid{grid_idx}.png")
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Plot saved as {plot_path}")





#----------PLOT LEARNING CURVE ALGORITHM----------#
def plot_algorithm(algorithm_name, grid_idx, variant, mean_rewards, std_rewards):
    """
    Plots the learning curve of a specific algorithm and variant for a given grid.

    Args:
        algorithm_name (str): Algorithm name (e.g., "q_learning", "montecarlo", "SARSA")
        grid_idx (int): Grid index
        variant (str): Variant name ('no_decay' or 'with_decay')
        mean_rewards (np.ndarray): Array of mean rewards per episode
        std_rewards (np.ndarray): Array of standard deviation of rewards per episode

    Saves:
        A PNG file of the plot named '{algorithm_name}_grid{grid_idx}_{variant}.png' 
        in the folder 'plots/{algorithm_name}'.
    """

    #Create the plot
    plt.figure(figsize=(8,4))
    plt.plot(mean_rewards, label=f"{variant}")
    plt.fill_between(
        np.arange(len(mean_rewards)),
        mean_rewards - std_rewards,
        mean_rewards + std_rewards,
        alpha=0.2
    )

    #Create the folder if it does not exist
    folder = f"plots/{algorithm_name}"
    os.makedirs(folder, exist_ok=True)

    #Extra settings for the plot
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Zero Line')
    plt.title(f"Algorithm {algorithm_name} - Grid {grid_idx} - {variant}")
    plt.xlabel("Episode")
    plt.ylabel("Average Reward")
    plt.legend()
    plt.tight_layout()
    plt.savefig(f"{folder}/{algorithm_name}_grid{grid_idx}_{variant}.png")
    plt.close()




def save_reward_plots(results_df, subfolder):
    """
    Generates and saves the plots with the results got from sensitivity analysis.

    
    Args:
    - results_df (pd.Dataframe): DataFrame with the results.
    - subfolder (str): Subdirectory name inside "plots" folder.
    """

    #Create the folder if it does not exist
    folder_path = os.path.join("plots", subfolder)
    os.makedirs(folder_path, exist_ok=True)

    # -----------------------------
    #Barplot: reward per configuration and algorithm
    plt.figure(figsize=(12,6))
    sns.barplot(
        data=results_df, 
        x='config', 
        y='mean_reward', 
        hue='algorithm',
        ci='sd'
    )
    plt.xticks(rotation=45)
    plt.ylabel("Average Reward")
    plt.title("Reward Comparison by Algorithm and Configuration")
    plt.legend(title="Algorithm")
    plt.tight_layout()
    plt.savefig(os.path.join(folder_path, "reward_comparison.png"))
    plt.close()

    # -----------------------------
    #Scatter: success vs steps, size = reward
    plt.figure(figsize=(10,6))
    sns.scatterplot(
        data=results_df,
        x='mean_steps',
        y='success_rate',
        hue='algorithm',
        size='mean_reward',
        sizes=(50, 300),
        alpha=0.8
    )
    plt.xlabel("Mean Steps")
    plt.ylabel("Success Rate")
    plt.title("Success Rate vs Mean Steps (size = avg reward)")
    plt.legend(title="Algorithm", bbox_to_anchor=(1.05, 1), loc=2)
    plt.tight_layout()
    plt.savefig(os.path.join(folder_path, "success_vs_steps.png"))
    plt.close()


    print(f"Plots saved in folder: {folder_path}")






#-----------------------------------------------#
#--------------APPLY EACH ALGORITHM-------------#
#-----------------------------------------------#

#----------EXECUTE EXPERIMENT----------#
def run_experiment(env, algorithm, episodes, alpha, gamma, epsilon,
                   epsilon_decay=None, epsilon_min=None, seed=None,
                   max_steps_episode=None):
    """
    Executes a given algorithm with the selected hyperparameters in all grids. It also saves calculated statistics in TXT file.

    Args:
        env (DaniEnv): The environment (matrix)
        algorithm (callable): Algorithm function (from algorithms.py) (e.g., q_learning, SARSA, montecarlo).
        episodes (int): Amount of episodes by execution
        alpha (float):  Learning rate
        gamma (float): Discount factor
        epsilon (float): Initial exploration rate for epsilon-greedy policy
        epsilon_decay (float, optional): Factor for epsilon decay. Default is None.
        epsilon_min (float, optional): Minimum allowed epsilon. Default is None.
        seed (int, optional): Seed for reproducibility. Default is None.
        max_steps_episode (int, optional): Maximum steps allowed per episode. Default is None.

    """

    variants = {
        "no_decay": {"epsilon_decay": None, "epsilon_min": None},
        "with_decay": {"epsilon_decay": epsilon_decay, "epsilon_min": epsilon_min}
    }

    for grid_idx in range(len(env.grid_list)):
        env.set_grid(grid_idx)
        print(f"\n===== GRID {grid_idx} =====")

        for variant, params in variants.items():
            print(f"Running variant: {variant}")
            start_time = time.time()
            
            Q, rewards, info = algorithm(
                env=env,
                alpha=alpha,
                gamma=gamma,
                epsilon=epsilon,
                episodes=episodes,
                epsilon_decay=params["epsilon_decay"],
                epsilon_min=params["epsilon_min"],
                seed=seed,
                max_steps_episode=max_steps_episode
            )

            rewards_all = np.array([rewards])
            lengths_all = np.array([info["lengths"]])
            Q_all = np.array([Q])

            elapsed = time.time() - start_time
            
            mean_rewards = np.mean(rewards_all, axis=0)
            std_rewards = np.std(rewards_all, axis=0)
            Q_avg = np.mean(Q_all, axis=0)

            #Statistics for last 100 episodes
            last100 = rewards_all[:, -100:]
            mean_last100 = np.mean(last100)
            std_last100 = np.std(last100)
            mean_len = np.mean(lengths_all[:, -100:])


            #Save them in the TXT
            save_results_txt(
                algorithm.__name__,
                grid_idx,
                variant,
                mean_last100,
                std_last100,
                mean_len,
                elapsed,
                mean_rewards,
                std_rewards
            )








#-----------------------------------------------#
#--------------------OPTUNA---------------------#
#-----------------------------------------------#

#------------Define hyperparameters-------------#
def objective_optuna(trial, env, algo, episodes, seed, max_steps_episode):
    """
    Objective function for Optuna hyperparameter optimization.
    Runs a given reinforcement learning algorithm on a given environment grid
    and returns the average reward over the last 100 episodes, which Optuna
    will try to maximize.

    Args:
        trial (optuna.trial.Trial): Optuna trial object for sampling hyperparameters
        env (DaniEnv): Environment object
        algo (callable): Algorithm function (e.g., q_learning, montecarlo, SARSA)
        episodes (int): Number of episodes to run
        seed (int): Random seed for reproducibility
        max_steps_episode (int): Maximum allowed steps per episode

    Returns:
        float: Average reward of the last 100 episodes
    """
    #Hyperparameters to optimize
    alpha = trial.suggest_float("alpha", 0.1, 0.9)
    gamma = trial.suggest_float("gamma", 0.8, 0.999)
    epsilon = trial.suggest_float("epsilon", 0.1, 1.0)
    use_decay = trial.suggest_categorical("use_decay", [True, False])

    if use_decay:
        epsilon_decay = trial.suggest_float("epsilon_decay", 0.95, 0.999)
        epsilon_min = trial.suggest_float("epsilon_min", 0.01, 0.1)
    else:
        epsilon_decay = None
        epsilon_min = None

    #Execute the algorithm
    _, rewards, _ = algo(
        env=env,
        alpha=alpha,
        gamma=gamma,
        epsilon=epsilon,
        episodes=episodes,
        epsilon_decay=epsilon_decay,
        epsilon_min=epsilon_min,
        seed=seed,
        max_steps_episode=max_steps_episode
    )

    #Return the average of the last 100 episodes
    return np.mean(rewards[-100:])



#--------------Try hyperparameters--------------#
def optimize_with_optuna(env, algos, episodes, seed, n_trials, max_steps_episode, n_jobs=1):
    """
    Optimize hyperparameters for multiple algorithms and grids using Optuna.
    Iterates over all algorithms and environment grids, performs hyperparameter
    optimization using Optuna, and stores the best parameters for each grid.

    Args:
        env (DaniEnv): Environment object containing multiple grids
        algos (list[callable]): List of algorithm functions to optimize
        episodes (int): Number of episodes for each trial
        seed (int): Random seed for reproducibility
        n_trials (int): Number of Optuna trials per grid
        max_steps_episode (int): Maximum steps allowed per episode
        n_jobs (int, optional): Number of CPU cores to use in parallel (default=1)

    Returns:
        dict: Nested dictionary with structure {algorithm_name: {grid_idx: best_params}}.
    """

    #Dictionary for the best obtained hyperparameters
    best_params_dict = {}

    for algo in algos:
        best_params_dict[algo.__name__] = {}
        for grid_idx in range(len(env.grid_list)):
            print(f"\n=== Optimizing {algo.__name__} on Grid {grid_idx} ===")
            env.set_grid(grid_idx)

            study = optuna.create_study(direction="maximize")
            
            study.optimize(
                lambda trial: objective_optuna(trial, env, algo, episodes, seed, max_steps_episode),
                n_trials=n_trials,
                n_jobs=n_jobs
            )

            best_params_dict[algo.__name__][grid_idx] = study.best_trial.params
            print(f"Best params for {algo.__name__}, Grid {grid_idx}: {study.best_trial.params}")

    return best_params_dict




#-----------Save best hyperparameters-----------#
def save_best_params(best_params, filename="best_params.json"): 
    """
    Save the best hyperparameters to a JSON file.

    Args:
        best_params (dict): Dictionary of best hyperparameters, e.g., {algorithm_name: {grid_idx: params}}.
        filename (str, optional): File path to save the JSON (default="best_params.json").
    """
    
    
    with open(filename, "w") as f:
        json.dump(best_params, f, indent=4)
    print(f"Best params saved to {filename}")




#-----------Load best hyperparameters-----------#
def load_best_params(filename="best_params.json"):
    """
    Load best hyperparameters previously saved in a JSON file.

    Args:
        filename (str, optional): Path to the JSON file (default="best_params.json").

    Returns:
        dict or None: Dictionary with best parameters if the file exists, 
                      None otherwise.
    """

    if os.path.exists(filename):
        with open(filename, "r") as f:
            best_params = json.load(f)
        print(f"\nLoaded best params from {filename}")
        return best_params
    else:
        print(f"No file found at {filename}")
        return None









#-----------------------------------------------#
#------Reward value tuning for algorithms-------#
#-----------------------------------------------#

def analyze_reward_sensitivity(algos, slippery=False):
    """
    Analyzes how reward configurations affect the performance of algorithms.

    Args:
        algos (list[callable]): List of algorithm functions to evaluate 
            (e.g., [q_learning, sarsa, montecarlo]).

        slippery (bool, optional): If True, introduces randomness in agent movement 
            to simulate a slippery environment (default=False).

    Returns:
        pd.DataFrame: A DataFrame summarizing the results of the sensitivity analysis.
            Columns include:
                - 'config': Name of the reward configuration tested
                - 'algorithm': Algorithm name
                - 'mean_reward': Average reward across all episodes
                - 'std_reward': Standard deviation of rewards
                - 'success_rate': Proportion of successful episodes
                - 'mean_steps': Average number of steps per episode
                - 'meteorite_reward', 'palm_reward', 'goal_reward', 'step_reward': 
                  Reward values used in the configuration
    """

    #Load best parameters from Optuna
    best_params = load_best_params()

    #Reward configurations to test
    reward_configs = [
        #Base configuration
        {
            'name': 'Base',
            'meteorite': -100,
            'palm': 20, 
            'goal': 100,
            'step': -1
        },
        #More negative meteorite
        {
            'name': 'Strong_Meteorite',
            'meteorite': -200,
            'palm': 20,
            'goal': 100,
            'step': -1
        },
        #Less negative meteorite
        {
            'name': 'Weak_Meteorite',
            'meteorite': -50,
            'palm': 20,
            'goal': 100,
            'step': -1
        },
        #More positive palm tree
        {
            'name': 'Strong_Palm',
            'meteorite': -100,
            'palm': 50,
            'goal': 100,
            'step': -1
        },
        #Less positive palm tree
        {
            'name': 'Weak_Palm',
            'meteorite': -100,
            'palm': 10,
            'goal': 100,
            'step': -1
        },
        #More positive goal
        {
            'name': 'Strong_Goal',
            'meteorite': -100,
            'palm': 20,
            'goal': 200,
            'step': -1
        },
        #Less positive goal
        {
            'name': 'Weak_Goal',
            'meteorite': -100,
            'palm': 20,
            'goal': 50,
            'step': -1
        },
        #More negative step
        {
            'name': 'Costly_Step',
            'meteorite': -100,
            'palm': 20,
            'goal': 100,
            'step': -5
        },
        #Less negative step
        {
            'name': 'Cheap_Step',
            'meteorite': -100,
            'palm': 20,
            'goal': 100,
            'step': -0.5
        }
    ]
    
    
    #Metrics
    results = []

    #Fixed grid for analysis
    grid_idx = 1
    episodes = 500
    max_steps = 200

    print("\n=== Reward Sensitivity Analysis ===")


    for config in reward_configs:
        print(f"Testing configuration: {config['name']}")

        env = DaniEnv()
        env.set_grid(grid_idx)

        def custom_step(action):
            i, j = env.agent_pos

            #If slippery, get the action with deviation
            if slippery:
                slip_prob = 0.2
                actions = [action, (action + 1) % 4, (action - 1) % 4]
                probs = [1 - slip_prob, slip_prob / 2, slip_prob / 2]
                action = np.random.choice(actions, p=probs)

            #Deterministic movement
            if action == 0 and i > 0: i -= 1
            if action == 1 and j < env.grid_size - 1: j += 1
            if action == 2 and i < env.grid_size - 1: i += 1
            if action == 3 and j > 0: j -= 1
            env.agent_pos = (i, j)

            cell = env.grid[i, j]
            reward, done = 0, False

            if cell == 'M':  
                reward, done = config['meteorite'], True
            elif cell == 'G':  
                reward, done = config['goal'], True
            elif cell == 'P':  
                reward = config['palm']
                env.grid[i, j] = 'N'
            else:  
                reward = config['step']

            return env._get_state(), reward, done, False, {}

        env.step = custom_step
        
        #Test each algorithm
        for algo in algos:
            algo_name = algo.__name__
            print(f"  - Running {algo_name}...")

            #Get best parameters for this algorithm and grid
            params = best_params[algo_name][str(grid_idx)]

            #Run algorithm
            Q, rewards, info = algo(
                env=env,
                alpha=params['alpha'],
                gamma=params['gamma'],
                epsilon=params['epsilon'],
                episodes=episodes,
                epsilon_decay=params.get('epsilon_decay'),
                epsilon_min=params.get('epsilon_min'),
                seed=100,
                max_steps_episode=max_steps
            )

            #Calculate metrics
            mean_reward = np.mean(rewards)
            std_reward = np.std(rewards)
            success_rate = np.mean([1 if r > 0 else 0 for r in rewards])
            mean_steps = np.mean(info["lengths"])

            #Save results
            results.append({
                'config': config['name'],
                'algorithm': algo_name, 
                'mean_reward': mean_reward,
                'std_reward': std_reward,
                'success_rate': success_rate,
                'mean_steps': mean_steps,
                'meteorite_reward': config['meteorite'],
                'palm_reward': config['palm'],
                'goal_reward': config['goal'],
                'step_reward': config['step']
            })
        print("")
        
        env.close()
    
    return pd.DataFrame(results)



